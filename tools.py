import asyncio
import json
import os
import re
import difflib
import shlex
import typing as t
from typing import cast, List
from urllib.parse import urlparse

import aiohttp
import aiofiles
from bs4 import BeautifulSoup
from ddgs import DDGS
from ollama import ChatResponse

# –ò–º–ø–æ—Ä—Ç—ã –∏–∑ –Ω–∞—à–∏—Ö –º–æ–¥—É–ª–µ–π
from config import *
import bd

# --- –ò–ù–°–¢–†–£–ú–ï–ù–¢–´ (TOOLS) ---

tools_definition_dev = [
    {
        "type": "function",
        "function": {
            "name": "get_project_info",
            "description": "–ò–Ω—Ñ–æ –æ –ø—Ä–æ–µ–∫—Ç–µ",
            "parameters": {"type": "object", "properties": {}},
        },
    },
    {
        "type": "function",
        "function": {
            "name": "update_project_plan",
            "description": "–û–±–Ω–æ–≤–∏—Ç—å –ø–ª–∞–Ω. –ò—Å–ø–æ–ª—å–∑—É–π —ç—Ç–æ, —á—Ç–æ–±—ã –æ—Ç–º–µ—Ç–∏—Ç—å –≤—ã–ø–æ–ª–Ω–µ–Ω–Ω—ã–µ –ø—É–Ω–∫—Ç—ã –∫–∞–∫ [–í–´–ü–û–õ–ù–ï–ù–û] –∏–ª–∏ [DONE].",
            "parameters": {"type": "object", "properties": {"plan": {"type": "string"}}},
        },
    },
    {
        "type": "function",
        "function": {
            "name": "write_file",
            "description": "–ó–∞–ø–∏—Å–∞—Ç—å —Ñ–∞–π–ª",
            "parameters": {
                "type": "object",
                "properties": {"path": {"type": "string"}, "content": {"type": "string"}},
                "required": ["path", "content"],
            },
        },
    },
    {
        "type": "function",
        "function": {
            "name": "read_file",
            "description": "–ß–∏—Ç–∞—Ç—å —Ñ–∞–π–ª",
            "parameters": {"type": "object", "properties": {"path": {"type": "string"}}, "required": ["path"]},
        },
    },
    {
        "type": "function",
        "function": {
            "name": "search_code",
            "description": "–ü–æ–∏—Å–∫ –≤ –∫–æ–¥–µ",
            "parameters": {"type": "object", "properties": {"query": {"type": "string"}}, "required": ["query"]},
        },
    },
    {
        "type": "function",
        "function": {
            "name": "search_docs",
            "description": "–ü–æ–∏—Å–∫ –≤ –∫–∞—Ç–∞–ª–æ–≥–µ –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏–∏",
            "parameters": {"type": "object", "properties": {"query": {"type": "string"}}, "required": ["query"]},
        },
    },
    {
        "type": "function",
        "function": {
            "name": "run_shell_command",
            "description": "–ö–æ–Ω—Å–æ–ª—å",
            "parameters": {"type": "object", "properties": {"command": {"type": "string"}}, "required": ["command"]},
        },
    },
    {
        "type": "function",
        "function": {
            "name": "scan_directory",
            "description": "–°–∫–∞–Ω–∏—Ä—É–µ—Ç –í–°–ï —Ñ–∞–π–ª—ã –ø—Ä–æ–µ–∫—Ç–∞ (.rs, .py, .tom–ª) –∏ –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç –∏—Ö –∫–æ–¥.",
            "parameters": {"type": "object", "properties": {}},
        },
    },
    {
        "type": "function",
        "function": {
            "name": "web_search",
            "description": "–ü–æ–∏—Å–∫ –≤ –∏–Ω—Ç–µ—Ä–Ω–µ—Ç–µ",
            "parameters": {"type": "object", "properties": {"query": {"type": "string"}}, "required": ["query"]},
        },
    },
]

tools_definition_analyzer = [
    {
        "type": "function",
        "function": {
            "name": "get_project_info",
            "description": "–ò–Ω—Ñ–æ –æ –ø—Ä–æ–µ–∫—Ç–µ",
            "parameters": {"type": "object", "properties": {}},
        },
    },
    {
        "type": "function",
        "function": {
            "name": "search_docs",
            "description": "–ü–æ–∏—Å–∫ –≤ –∫–∞—Ç–∞–ª–æ–≥–µ –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏–∏",
            "parameters": {"type": "object", "properties": {"query": {"type": "string"}}, "required": ["query"]},
        },
    },
    {
        "type": "function",
        "function": {
            "name": "scan_directory",
            "description": "–°–∫–∞–Ω–∏—Ä—É–µ—Ç —Ñ–∞–π–ª—ã –ø—Ä–æ–µ–∫—Ç–∞",
            "parameters": {"type": "object", "properties": {}},
        },
    },
    {
        "type": "function",
        "function": {
            "name": "web_search",
            "description": "–ü–æ–∏—Å–∫ –≤ –∏–Ω—Ç–µ—Ä–Ω–µ—Ç–µ",
            "parameters": {"type": "object", "properties": {"query": {"type": "string"}}, "required": ["query"]},
        },
    },
]

tools_definition_dialog_web = [
    {
        "type": "function",
        "function": {
            "name": "web_search",
            "description": "–ü–æ–∏—Å–∫ –≤ –∏–Ω—Ç–µ—Ä–Ω–µ—Ç–µ –¥–ª—è –ø–æ–ª—É—á–µ–Ω–∏—è –∞–∫—Ç—É–∞–ª—å–Ω–æ–π –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏",
            "parameters": {"type": "object", "properties": {"query": {"type": "string"}}, "required": ["query"]},
        },
    },
]


# --- –ò–ù–°–¢–†–£–ú–ï–ù–¢–´ –ê–ì–ï–ù–¢–ê ---

def get_full_path(rel_path: str) -> str:
    """–ë–µ–∑–æ–ø–∞—Å–Ω–æ–µ –ø–æ–ª—É—á–µ–Ω–∏–µ –∞–±—Å–æ–ª—é—Ç–Ω–æ–≥–æ –ø—É—Ç–∏ –≤ —Ä–∞–º–∫–∞—Ö –ø—Ä–æ–µ–∫—Ç–∞"""
    if not ACTIVE_PROJECT or not ACTIVE_PROJECT.get("path"):
        raise PermissionError("–ù–µ—Ç –∞–∫—Ç–∏–≤–Ω–æ–≥–æ –ø—Ä–æ–µ–∫—Ç–∞.")

    base_path = os.path.abspath(ACTIVE_PROJECT["path"])
    rel_path = rel_path.strip()

    if os.path.isabs(rel_path):
        target_path = os.path.abspath(rel_path)
    else:
        target_path = os.path.abspath(os.path.join(base_path, rel_path))

    if not target_path.startswith(base_path):
        raise PermissionError(f"–í—ã—Ö–æ–¥ –∑–∞ –ø—Ä–µ–¥–µ–ª—ã –ø—Ä–æ–µ–∫—Ç–∞: {rel_path}")

    return target_path


async def scan_directory_tool() -> str:
    """–°–∫–∞–Ω–∏—Ä—É–µ—Ç –≤—Å–µ –≤–∞–∂–Ω—ã–µ —Ñ–∞–π–ª—ã –ø—Ä–æ–µ–∫—Ç–∞"""
    if not ACTIVE_PROJECT:
        return "–ù–µ—Ç –ø—Ä–æ–µ–∫—Ç–∞."

    if not ACTIVE_PROJECT.get("path"):
        return "–ü—É—Ç—å –∫ –ø—Ä–æ–µ–∫—Ç—É –Ω–µ —É–∫–∞–∑–∞–Ω –≤ –±–∞–∑–µ –¥–∞–Ω–Ω—ã—Ö."

    base_path = os.path.abspath(ACTIVE_PROJECT["path"])
    print(f"{C_GRAY}[SCAN]{C_RESET} –°–∫–∞–Ω–∏—Ä—É—é {base_path}...")

    cmd = (
        f"find {shlex.quote(base_path)} -type f "
        r'\( -name "*.rs" -o -name "*.py" -o -name "*.toml" -o -name "*.txt" -o -name "*.yaml" -o -name "*.yml" -o -name "*.md" \) '
        r'-not -path "*/target/*" -not -path "*/node_modules/*" -not -path "*/.venv/*" -not -path "*/__pycache__/*" -not -path "*/.git/*"'
    )

    try:
        proc = await asyncio.create_subprocess_shell(
            cmd,
            stdout=asyncio.subprocess.PIPE,
            stderr=asyncio.subprocess.PIPE,
        )
        stdout, stderr = await asyncio.wait_for(proc.communicate(), timeout=30.0)

        if stderr and b"Permission denied" in stderr:
            print(f"{C_YELLOW}[WARN]{C_RESET} –ù–µ—Ç –¥–æ—Å—Ç—É–ø–∞ –∫ –Ω–µ–∫–æ—Ç–æ—Ä—ã–º —Ñ–∞–π–ª–∞–º.")

        file_paths = [f.strip() for f in stdout.decode().splitlines()]

        if not file_paths:
            return "–§–∞–π–ª–æ–≤ –Ω–µ –Ω–∞–π–¥–µ–Ω–æ."

        print(f"{C_GRAY}[SCAN]{C_RESET} –ù–∞–π–¥–µ–Ω–æ {len(file_paths)} —Ñ–∞–π–ª–æ–≤.")

        combined_text = "--- –°–û–î–ï–†–ñ–ò–ú–û–ï –ü–†–û–ï–ö–¢–ê ---\n"

        for f_path in file_paths:
            relative_name = ""
            try:
                relative_name = os.path.relpath(f_path, base_path)
                async with aiofiles.open(f_path, "r", encoding="utf-8", errors="replace") as f:
                    content = await f.read()
                    preview = content[:LENGTH_CONTEXT] + "... (–æ–±—Ä–µ–∑–∞–Ω–æ)" if len(content) > LENGTH_CONTEXT else content
                    combined_text += f"\n>>> FILE: {relative_name} <<<\n{preview}\n"
            except Exception as e:
                combined_text += f"\n>>> FILE: {relative_name} <<<\n–û–®–ò–ë–ö–ê –ß–¢–ï–ù–ò–Ø: {e}\n"

        return combined_text
    except asyncio.TimeoutError:
        return f"{C_RED}–û—à–∏–±–∫–∞ —Å–∫–∞–Ω–∏—Ä–æ–≤–∞–Ω–∏—è: —Ç–∞–π–º–∞—É—Ç{C_RESET}"
    except Exception as e:
        return f"–û—à–∏–±–∫–∞ —Å–∫–∞–Ω–∏—Ä–æ–≤–∞–Ω–∏—è: {e}"


async def get_dialog_status() -> str:
    """–ü–æ–∫–∞–∑—ã–≤–∞–µ—Ç —Å—Ç–∞—Ç—É—Å –≥–ª–æ–±–∞–ª—å–Ω–æ–≥–æ –¥–∏–∞–ª–æ–≥–∞"""
    if not bd.r:
        return "Redis –Ω–µ –¥–æ—Å—Ç—É–ø–µ–Ω."

    try:
        length = await cast(t.Awaitable[int], bd.r.llen(REDIS_DIALOG_KEY))
        memory_usage = await bd.r.memory_usage(REDIS_DIALOG_KEY) or 0

        preview = ""
        if length > 0:
            first_msgs = await cast(t.Awaitable[List[str]], bd.r.lrange(REDIS_DIALOG_KEY, 0, 2))
            last_msgs = await cast(t.Awaitable[List[str]], bd.r.lrange(REDIS_DIALOG_KEY, -3, -1))

            preview += f"\n{C_GRAY}–ü–µ—Ä–≤—ã–µ —Å–æ–æ–±—â–µ–Ω–∏—è:{C_RESET}\n"
            for i, msg in enumerate(first_msgs, 1):
                try:
                    data = json.loads(msg)
                    role = data.get("role", "unknown")
                    content = data.get("content", "")[:50] + "..." if len(data.get("content", "")) > 50 else data.get("content", "")
                    preview += f"  {i}. [{role}] {content}\n"
                except:
                    preview += f"  {i}. [–æ—à–∏–±–∫–∞ —á—Ç–µ–Ω–∏—è]\n"

            if length > 6:
                preview += f"  ... ({length - 6} —Å–æ–æ–±—â–µ–Ω–∏–π —Å–∫—Ä—ã—Ç–æ) ...\n"

            preview += f"\n{C_GRAY}–ü–æ—Å–ª–µ–¥–Ω–∏–µ —Å–æ–æ–±—â–µ–Ω–∏—è:{C_RESET}\n"
            for i, msg in enumerate(last_msgs, max(1, length - 2)):
                try:
                    data = json.loads(msg)
                    role = data.get("role", "unknown")
                    content = data.get("content", "")[:50] + "..." if len(data.get("content", "")) > 50 else data.get("content", "")
                    preview += f"  {i}. [{role}] {content}\n"
                except:
                    preview += f"  {i}. [–æ—à–∏–±–∫–∞ —á—Ç–µ–Ω–∏—è]\n"

        return (f"{C_CYAN}=== –°—Ç–∞—Ç—É—Å –¥–∏–∞–ª–æ–≥–∞ ==={C_RESET}\n"
                f"–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —Å–æ–æ–±—â–µ–Ω–∏–π: {C_GREEN}{length}{C_RESET}\n"
                f"–ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ –ø–∞–º—è—Ç–∏: {C_GREEN}{memory_usage / 1024:.2f} KB{C_RESET}\n"
                f"–õ–∏–º–∏—Ç –∏—Å—Ç–æ—Ä–∏–∏: {MAX_DIALOG_HISTORY}\n"
                f"{preview}")
    except Exception as e:
        return f"{C_RED}–û—à–∏–±–∫–∞ –ø–æ–ª—É—á–µ–Ω–∏—è —Å—Ç–∞—Ç—É—Å–∞: {e}{C_RESET}"


async def clean_dialog_history() -> str:
    """–û—á–∏—â–∞–µ—Ç –≤—Å—é –∏—Å—Ç–æ—Ä–∏—é –≥–ª–æ–±–∞–ª—å–Ω–æ–≥–æ –¥–∏–∞–ª–æ–≥–∞"""
    if not bd.r:
        return f"{C_RED}Redis –Ω–µ –¥–æ—Å—Ç—É–ø–µ–Ω.{C_RESET}"

    try:
        await bd.r.delete(REDIS_DIALOG_KEY)
        return f"{C_GREEN}‚úÖ{C_RESET} –ò—Å—Ç–æ—Ä–∏—è –¥–∏–∞–ª–æ–≥–∞ –ø–æ–ª–Ω–æ—Å—Ç—å—é –æ—á–∏—â–µ–Ω–∞."
    except Exception as e:
        return f"{C_RED}–û—à–∏–±–∫–∞ –æ—á–∏—Å—Ç–∫–∏: {e}{C_RESET}"


async def dialog_web_loop(user_input: str):
    """–ì–ª–æ–±–∞–ª—å–Ω—ã–π –¥–∏–∞–ª–æ–≥ —Å –≤–µ–±-–ø–æ–∏—Å–∫–æ–º –∏ –ø–æ–¥–¥–µ—Ä–∂–∫–æ–π –º–Ω–æ–∂–µ—Å—Ç–≤–µ–Ω–Ω—ã—Ö tool_calls"""
    global r, client

    if not bd.r or not bd.client:
        print(f"{C_RED}[ERROR]{C_RESET} –°–∏—Å—Ç–µ–º–∞ –Ω–µ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–∞.")
        return

    tools = tools_definition_dialog_web
    messages = [{"role": "system", "content": SYSTEM_PROMPT_DIALOG_WEB}]

    # –ó–∞–≥—Ä—É–∂–∞–µ–º –∏—Å—Ç–æ—Ä–∏—é –∏–∑ Redis
    previous_msgs = await cast(t.Awaitable[List[str]], bd.r.lrange(REDIS_DIALOG_KEY, -MAX_DIALOG_HISTORY, -1))
    if previous_msgs:
        try:
            history = [json.loads(m) for m in previous_msgs]
            messages.extend(history)
            print(f"{C_GRAY}[CONTEXT]{C_RESET} –ó–∞–≥—Ä—É–∂–µ–Ω–æ {len(history)} —Å–æ–æ–±—â–µ–Ω–∏–π –∏–∑ –∏—Å—Ç–æ—Ä–∏–∏ –¥–∏–∞–ª–æ–≥–æ–≤.")
        except json.JSONDecodeError:
            pass

    # –î–æ–±–∞–≤–ª—è–µ–º —Å–æ–æ–±—â–µ–Ω–∏–µ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è
    await cast(t.Awaitable[int], bd.r.rpush(REDIS_DIALOG_KEY, json.dumps({"role": "user", "content": user_input})))
    messages.append({"role": "user", "content": user_input})

    # –û–°–ù–û–í–ù–û–ô –¶–ò–ö–õ - –ø–æ–¥–¥–µ—Ä–∂–∫–∞ –º–Ω–æ–∂–µ—Å—Ç–≤–µ–Ω–Ω—ã—Ö tool_calls
    max_iterations = DIALOG_MAX_ITERATIONS
    for iteration in range(max_iterations):
        print(f"{C_GRAY}[DIALOG]{C_RESET} –ò—Ç–µ—Ä–∞—Ü–∏—è {iteration + 1}/{max_iterations}...")

        try:
            response: ChatResponse = await bd.client.chat(
                model=OLLAMA_MODEL,
                messages=messages,
                tools=tools,
                options=OLLAMA_OPTIONS
            )
        except Exception as e:
            print(f"{C_RED}[ERROR]{C_RESET} –û—à–∏–±–∫–∞ Ollama: {e}")
            return

        msg = response["message"]

        # –ï—Å–ª–∏ –º–æ–¥–µ–ª—å —Ö–æ—á–µ—Ç –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç—ã
        if msg.get("tool_calls"):
            try:
                msg_dict = msg.model_dump() if hasattr(msg, "model_dump") else dict(msg)
            except:
                msg_dict = dict(msg)

            # –°–æ—Ö—Ä–∞–Ω—è–µ–º —Å–æ–æ–±—â–µ–Ω–∏–µ —Å tool_calls
            await cast(t.Awaitable[int], bd.r.rpush(REDIS_DIALOG_KEY, json.dumps(msg_dict)))
            messages.append(msg_dict)

            # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º –∫–∞–∂–¥—ã–π –≤—ã–∑–æ–≤ –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç–∞
            for tool in msg.get("tool_calls"):
                fn = tool.get("function", {})
                name = fn.get("name")
                args = fn.get("arguments", {}) or {}
                tool_id = tool.get("id") or f"{name}_{hash(str(args))}" or "unknown"

                if name == "web_search":
                    query = args.get("query")
                    if isinstance(query, str):
                        print(f"{C_CYAN}[WEB]{C_RESET} üîç –ü–æ–∏—Å–∫ #{iteration + 1}: {query}")
                        res = await web_search_tool(query)
                    else:
                        res = "–û—à–∏–±–∫–∞: –Ω–µ–≤–µ—Ä–Ω—ã–π –∑–∞–ø—Ä–æ—Å"
                else:
                    res = f"–ò–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç {name} –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω –≤ —Ä–µ–∂–∏–º–µ –¥–∏–∞–ª–æ–≥–∞"

                # –°–æ—Ö—Ä–∞–Ω—è–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç–∞
                tool_result = {
                    "role": "tool",
                    "content": res,
                    "tool_call_id": tool_id,
                    "name": name,
                }
                await cast(t.Awaitable[int], bd.r.rpush(REDIS_DIALOG_KEY, json.dumps(tool_result)))
                messages.append(tool_result)

            # –ü—Ä–æ–¥–æ–ª–∂–∞–µ–º —Ü–∏–∫–ª - –¥–∞—ë–º –º–æ–¥–µ–ª–∏ –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç—å –æ–±—Ä–∞–±–æ—Ç–∞—Ç—å —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
            continue

        # –ï—Å–ª–∏ –º–æ–¥–µ–ª—å –≤–µ—Ä–Ω—É–ª–∞ —Ç–µ–∫—Å—Ç–æ–≤—ã–π –æ—Ç–≤–µ—Ç
        text = msg.get("content", "")
        if text:
            print(f"{C_GREEN}ü§ñ [DIALOG]:{C_RESET} {text}")
            await cast(t.Awaitable[int], bd.r.rpush(REDIS_DIALOG_KEY, json.dumps({"role": "assistant", "content": text})))
            break
        else:
            # –ù–µ—Ç –Ω–∏ tool_calls, –Ω–∏ content
            print(f"{C_YELLOW}[WARN]{C_RESET} –ú–æ–¥–µ–ª—å –≤–µ—Ä–Ω—É–ª–∞ –ø—É—Å—Ç–æ–π –æ—Ç–≤–µ—Ç –Ω–∞ –∏—Ç–µ—Ä–∞—Ü–∏–∏ {iteration + 1}")
            if iteration == max_iterations - 1:
                fallback_text = "–ò–∑–≤–∏–Ω–∏—Ç–µ, –Ω–µ —É–¥–∞–ª–æ—Å—å —Å—Ñ–æ—Ä–º–∏—Ä–æ–≤–∞—Ç—å –æ—Ç–≤–µ—Ç. –ü–æ–ø—Ä–æ–±—É–π—Ç–µ –ø–µ—Ä–µ—Ñ–æ—Ä–º—É–ª–∏—Ä–æ–≤–∞—Ç—å –≤–æ–ø—Ä–æ—Å."
                print(f"{C_GREEN}ü§ñ [DIALOG]:{C_RESET} {fallback_text}")
                await cast(t.Awaitable[int], bd.r.rpush(REDIS_DIALOG_KEY, json.dumps({"role": "assistant", "content": fallback_text})))
            break

    # –û–±—Ä–µ–∑–∞–µ–º –∏—Å—Ç–æ—Ä–∏—é –µ—Å–ª–∏ –æ–Ω–∞ —Å–ª–∏—à–∫–æ–º –¥–ª–∏–Ω–Ω–∞—è
    current_len = await cast(t.Awaitable[int], bd.r.llen(REDIS_DIALOG_KEY))
    if current_len > MAX_DIALOG_HISTORY * 2:
        await bd.r.ltrim(REDIS_DIALOG_KEY, -MAX_DIALOG_HISTORY, -1)
        print(f"{C_GRAY}[REDIS]{C_RESET} –ò—Å—Ç–æ—Ä–∏—è –æ–±—Ä–µ–∑–∞–Ω–∞ –¥–æ {MAX_DIALOG_HISTORY} —Å–æ–æ–±—â–µ–Ω–∏–π.")

async def search_docs_tool(query: str) -> str:
    """–ü–æ–∏—Å–∫ –≤ –∫–∞—Ç–∞–ª–æ–≥–µ –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏–∏"""
    if not ACTIVE_PROJECT or not ACTIVE_PROJECT.get("doc_path"):
        return "–ù–µ—Ç –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏–∏."

    doc_path = ACTIVE_PROJECT["doc_path"]
    if not os.path.exists(doc_path):
        return f"–ö–∞—Ç–∞–ª–æ–≥ –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏–∏ –Ω–µ –Ω–∞–π–¥–µ–Ω: {doc_path}"

    print(f"{C_GRAY}[DOCS]{C_RESET} –ü–æ–∏—Å–∫: {query}")
    try:
        proc = await asyncio.create_subprocess_shell(
            f"rga -i -n {shlex.quote(query)} {shlex.quote(doc_path)}",
            stdout=asyncio.subprocess.PIPE,
            stderr=asyncio.subprocess.PIPE,
        )
        stdout, _ = await asyncio.wait_for(proc.communicate(), timeout=20.0)
        result = stdout.decode()
        return result[:4000] if result else "–ù–µ –Ω–∞–π–¥–µ–Ω–æ."
    except asyncio.TimeoutError:
        return "–¢–∞–π–º–∞—É—Ç –ø–æ–∏—Å–∫–∞."
    except Exception:
        return "–û—à–∏–±–∫–∞ –ø–æ–∏—Å–∫–∞."


async def search_code_tool(query: str) -> str:
    """–ü–æ–∏—Å–∫ –≤ –∫–æ–¥–µ –ø—Ä–æ–µ–∫—Ç–∞"""
    if not ACTIVE_PROJECT:
        return "–ù–µ—Ç –ø—Ä–æ–µ–∫—Ç–∞."

    path = ACTIVE_PROJECT["path"]
    print(f"{C_GRAY}[SEARCH]{C_RESET} –ü–æ–∏—Å–∫ –∫–æ–¥–∞: {query}")
    try:
        proc = await asyncio.create_subprocess_shell(
            f"rga -i -n --glob='!.git' {shlex.quote(query)} {shlex.quote(path)}",
            stdout=asyncio.subprocess.PIPE,
            stderr=asyncio.subprocess.PIPE,
        )
        stdout, _ = await asyncio.wait_for(proc.communicate(), timeout=20.0)
        result = stdout.decode()
        return result[:4000] if result else "–ù–µ –Ω–∞–π–¥–µ–Ω–æ."
    except asyncio.TimeoutError:
        return "–¢–∞–π–º–∞—É—Ç –ø–æ–∏—Å–∫–∞."
    except Exception:
        return "–û—à–∏–±–∫–∞ –ø–æ–∏—Å–∫–∞."


async def write_file_tool(path: str, content: str) -> str:
    """–ó–∞–ø–∏—Å—å —Ñ–∞–π–ª–∞ —Å –ø–æ–¥—Ç–≤–µ—Ä–∂–¥–µ–Ω–∏–µ–º –∏ diff"""
    try:
        full_path = get_full_path(path)

        diff_text = ""
        if os.path.exists(full_path):
            print(f"{C_YELLOW}[WARN]{C_RESET} –§–∞–π–ª '{path}' —Å—É—â–µ—Å—Ç–≤—É–µ—Ç. –ß–∏—Ç–∞—é —Å—Ç–∞—Ä—É—é –≤–µ—Ä—Å–∏—é...")
            async with aiofiles.open(full_path, "r", encoding="utf-8") as f:
                old_content = await f.read()

            diff = difflib.unified_diff(
                old_content.splitlines(keepends=True),
                content.splitlines(keepends=True),
                fromfile=f"a/{path}",
                tofile=f"b/{path}",
                lineterm="",
            )
            diff_text = "".join(diff)

            if diff_text:
                print(f"\n{C_GRAY}--- DIFF ---{C_RESET}")
                print(diff_text)
                print(f"{C_GRAY}--- END ---{C_RESET}")

            confirm = await asyncio.to_thread(input, f"{C_YELLOW}‚ùì –ü–µ—Ä–µ–∑–∞–ø–∏—Å–∞—Ç—å '{path}'? [y/N]: {C_RESET}")
            if confirm.lower() != "y":
                return "–ó–∞–ø–∏—Å—å –æ—Ç–º–µ–Ω–µ–Ω–∞."

        os.makedirs(os.path.dirname(full_path), exist_ok=True)
        async with aiofiles.open(full_path, "w", encoding="utf-8") as f:
            await f.write(content)
        return f"{C_GREEN}‚úÖ{C_RESET} –§–∞–π–ª –∑–∞–ø–∏—Å–∞–Ω: {path}"
    except PermissionError as e:
        return f"{C_RED}–û—à–∏–±–∫–∞: {e}{C_RESET}"
    except Exception as e:
        return f"{C_RED}–û—à–∏–±–∫–∞ –∑–∞–ø–∏—Å–∏: {e}{C_RESET}"


async def read_file_tool(path: str) -> str:
    """–ß—Ç–µ–Ω–∏–µ —Ñ–∞–π–ª–∞"""
    try:
        full_path = get_full_path(path)
        async with aiofiles.open(full_path, "r", encoding="utf-8", errors="replace") as f:
            return await f.read()
    except FileNotFoundError:
        return f"–§–∞–π–ª –Ω–µ –Ω–∞–π–¥–µ–Ω: {path}"
    except Exception as e:
        return f"–û—à–∏–±–∫–∞ —á—Ç–µ–Ω–∏—è: {e}"


async def run_shell_tool(cmd: str) -> str:
    """–í—ã–ø–æ–ª–Ω–µ–Ω–∏–µ shell-–∫–æ–º–∞–Ω–¥—ã –≤ –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏ –ø—Ä–æ–µ–∫—Ç–∞"""
    if not ACTIVE_PROJECT:
        return "–ù–µ—Ç –ø—Ä–æ–µ–∫—Ç–∞."

    project_path = ACTIVE_PROJECT["path"]
    print(f"{C_GRAY}[SHELL]{C_RESET} –ö–æ–º–∞–Ω–¥–∞: {cmd}")
    try:
        proc = await asyncio.create_subprocess_shell(
            cmd,
            cwd=project_path,
            stdout=asyncio.subprocess.PIPE,
            stderr=asyncio.subprocess.PIPE,
        )
        stdout, stderr = await asyncio.wait_for(proc.communicate(), timeout=60.0)

        out = []
        if stdout:
            out.append(f"STDOUT:\n{stdout.decode()}")
        if stderr:
            out.append(f"STDERR:\n{stderr.decode()}")

        result = "\n".join(out)
        return result[:4000] if result else "–ö–æ–º–∞–Ω–¥–∞ –≤—ã–ø–æ–ª–Ω–µ–Ω–∞."
    except asyncio.TimeoutError:
        return f"{C_RED}–¢–∞–π–º–∞—É—Ç –∫–æ–º–∞–Ω–¥—ã{C_RESET}"
    except Exception as e:
        return f"{C_RED}–û—à–∏–±–∫–∞: {e}{C_RESET}"


async def web_search_tool(query: str) -> str:
    """–í–µ–±-–ø–æ–∏—Å–∫ —á–µ—Ä–µ–∑ DuckDuckGo —Å —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–µ–π –∫–∏—Ç–∞–π—Å–∫–∏—Ö –∏ –º—É—Å–æ—Ä–Ω—ã—Ö —Å–∞–π—Ç–æ–≤"""
    print(f"{C_GRAY}[WEB]{C_RESET} –ü–æ–∏—Å–∫: {query} (–º–∞–∫—Å. {WEB_SEARCH_MAX_RESULTS} —Å–∞–π—Ç–æ–≤)")

    loop = asyncio.get_running_loop()
    all_texts = []

    # –û–ø—Ä–µ–¥–µ–ª—è–µ–º —Å–ø–µ—Ü–∏–∞–ª—å–Ω—ã–µ —Å–ª—É—á–∞–∏ –¥–ª—è –ø—Ä—è–º–æ–≥–æ –∑–∞–ø—Ä–æ—Å–∞
    rust_query = 'rust' in query.lower() and ('–≤–µ—Ä—Å–∏—è' in query.lower() or 'version' in query.lower())
    python_query = 'python' in query.lower() and ('–≤–µ—Ä—Å–∏—è' in query.lower() or 'version' in query.lower())

    # –î–ª—è –∑–∞–ø—Ä–æ—Å–æ–≤ –æ –≤–µ—Ä—Å–∏—è—Ö —è–∑—ã–∫–æ–≤ –ø—Ä–æ–≥—Ä–∞–º–º–∏—Ä–æ–≤–∞–Ω–∏—è - —Å—Ä–∞–∑—É –∏–¥–µ–º –∫ –æ—Ñ–∏—Ü–∏–∞–ª—å–Ω—ã–º –∏—Å—Ç–æ—á–Ω–∏–∫–∞–º
    if rust_query:
        print(f"{C_CYAN}[DIRECT]{C_RESET} –ó–∞–ø—Ä–æ—Å –∫ –æ—Ñ–∏—Ü–∏–∞–ª—å–Ω–æ–º—É –∏—Å—Ç–æ—á–Ω–∏–∫—É Rust...")
        try:
            async with aiohttp.ClientSession() as session:
                # –ü–æ–ª—É—á–∞–µ–º –≥–ª–∞–≤–Ω—É—é —Å—Ç—Ä–∞–Ω–∏—Ü—É rust-lang.org
                try:
                    async with session.get('https://www.rust-lang.org/', timeout=aiohttp.ClientTimeout(total=10)) as resp:
                        html = await resp.text()
                        soup = BeautifulSoup(html, "html.parser")

                        # –ò—â–µ–º –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ –≤–µ—Ä—Å–∏–∏ –Ω–∞ –≥–ª–∞–≤–Ω–æ–π —Å—Ç—Ä–∞–Ω–∏—Ü–µ
                        text = soup.get_text(separator="\n", strip=True)
                        lines = [line.strip() for line in text.splitlines() if line.strip() and len(line.strip()) > 10]
                        text = "\n".join(lines[:100])  # –£–≤–µ–ª–∏—á–µ–Ω–æ —Å 50 –¥–æ 100 —Å—Ç—Ä–æ–∫

                        if text:
                            all_texts.append(f"=== –ò—Å—Ç–æ—á–Ω–∏–∫ 1: Rust Official Website (rust-lang.org) ===\n{text[:3500]}")  # –£–≤–µ–ª–∏—á–µ–Ω–æ —Å 2000 –¥–æ 3500
                            print(f"{C_GREEN}‚úì{C_RESET} –ü–æ–ª—É—á–µ–Ω–æ —Å rust-lang.org: {len(text[:3500])} —Å–∏–º–≤–æ–ª–æ–≤")
                except Exception as e:
                    print(f"{C_YELLOW}‚ö†{C_RESET} –û—à–∏–±–∫–∞ rust-lang.org: {e}")

                # –ü—Ä–æ–±—É–µ–º –ø–æ–ª—É—á–∏—Ç—å –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ —Ä–µ–ª–∏–∑–∞—Ö –∏–∑ –±–ª–æ–≥–∞
                try:
                    async with session.get('https://blog.rust-lang.org/', timeout=aiohttp.ClientTimeout(total=10)) as resp:
                        html = await resp.text()
                        soup = BeautifulSoup(html, "html.parser")

                        # –ò—â–µ–º –ø–æ—Å–ª–µ–¥–Ω–∏–µ –ø–æ—Å—Ç—ã –æ —Ä–µ–ª–∏–∑–∞—Ö
                        posts_found = 0
                        for article in soup.find_all(['article', 'div'], limit=10):
                            title_elem = article.find(['h1', 'h2', 'h3', 'a'])
                            if not title_elem:
                                continue

                            title = title_elem.get_text(strip=True)

                            if any(keyword in title.lower() for keyword in ['announcing', 'release', '1.', 'rust']):
                                content = article.get_text(separator="\n", strip=True)
                                lines = [line.strip() for line in content.splitlines() if line.strip() and len(line.strip()) > 10]
                                content = "\n".join(lines[:80])  # –£–≤–µ–ª–∏—á–µ–Ω–æ —Å 40 –¥–æ 80 —Å—Ç—Ä–æ–∫

                                if len(content) > 100:
                                    all_texts.append(f"=== –ò—Å—Ç–æ—á–Ω–∏–∫ {len(all_texts)+1}: Rust Blog - {title[:80]} ===\n{content[:3000]}")  # –£–≤–µ–ª–∏—á–µ–Ω–æ —Å 1800 –¥–æ 3000
                                    print(f"{C_GREEN}‚úì{C_RESET} –ü–æ–ª—É—á–µ–Ω–æ —Å blog.rust-lang.org: {title[:60]}... ({len(content[:3000])} —Å–∏–º–≤–æ–ª–æ–≤)")
                                    posts_found += 1

                                    if posts_found >= 2:  # –ë–µ—Ä–µ–º –º–∞–∫—Å–∏–º—É–º 2 –ø–æ—Å—Ç–∞ –æ —Ä–µ–ª–∏–∑–∞—Ö
                                        break
                except Exception as e:
                    print(f"{C_YELLOW}‚ö†{C_RESET} –û—à–∏–±–∫–∞ blog.rust-lang.org: {e}")

                # –ü—Ä–æ–±—É–µ–º –ø–æ–ª—É—á–∏—Ç—å changelog –∏–ª–∏ release notes
                try:
                    async with session.get('https://github.com/rust-lang/rust/releases', timeout=aiohttp.ClientTimeout(total=10)) as resp:
                        html = await resp.text()
                        soup = BeautifulSoup(html, "html.parser")

                        # –ò—â–µ–º –ø–µ—Ä–≤—ã–π —Ä–µ–ª–∏–∑
                        release = soup.find('div', class_='release-entry') or soup.find('section')
                        if release:
                            content = release.get_text(separator="\n", strip=True)
                            lines = [line.strip() for line in content.splitlines() if line.strip() and len(line.strip()) > 10]
                            content = "\n".join(lines[:60])  # –£–≤–µ–ª–∏—á–µ–Ω–æ —Å 30 –¥–æ 60 —Å—Ç—Ä–æ–∫

                            if len(content) > 100:
                                all_texts.append(f"=== –ò—Å—Ç–æ—á–Ω–∏–∫ {len(all_texts)+1}: GitHub Rust Releases ===\n{content[:2500]}")  # –£–≤–µ–ª–∏—á–µ–Ω–æ —Å 1500 –¥–æ 2500
                                print(f"{C_GREEN}‚úì{C_RESET} –ü–æ–ª—É—á–µ–Ω–æ —Å GitHub releases: {len(content[:2500])} —Å–∏–º–≤–æ–ª–æ–≤")
                except Exception as e:
                    print(f"{C_YELLOW}‚ö†{C_RESET} –û—à–∏–±–∫–∞ GitHub releases: {e}")

                if all_texts:
                    combined = "\n\n".join(all_texts)
                    if len(combined) > WEB_SEARCH_MAX_LENGTH:
                        combined = combined[:WEB_SEARCH_MAX_LENGTH] + f"\n\n... [–û–±—Ä–µ–∑–∞–Ω–æ –¥–æ {WEB_SEARCH_MAX_LENGTH} —Å–∏–º–≤–æ–ª–æ–≤]"
                    print(f"{C_GRAY}[WEB]{C_RESET} –í–æ–∑–≤—Ä–∞—â–∞—é {len(combined)} —Å–∏–º–≤–æ–ª–æ–≤ –¥–∞–Ω–Ω—ã—Ö –∏–∑ {len(all_texts)} –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤")
                    return combined
                else:
                    print(f"{C_YELLOW}[WARN]{C_RESET} –ü—Ä—è–º–æ–π –∑–∞–ø—Ä–æ—Å –Ω–µ –¥–∞–ª —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤, –ø–µ—Ä–µ–∫–ª—é—á–∞—é—Å—å –Ω–∞ –ø–æ–∏—Å–∫...")

        except Exception as e:
            print(f"{C_YELLOW}[WARN]{C_RESET} –ü—Ä—è–º–æ–π –∑–∞–ø—Ä–æ—Å –Ω–µ —É–¥–∞–ª—Å—è: {e}, –ø–µ—Ä–µ–∫–ª—é—á–∞—é—Å—å –Ω–∞ –ø–æ–∏—Å–∫...")

    elif python_query:
        print(f"{C_CYAN}[DIRECT]{C_RESET} –ó–∞–ø—Ä–æ—Å –∫ –æ—Ñ–∏—Ü–∏–∞–ª—å–Ω–æ–º—É –∏—Å—Ç–æ—á–Ω–∏–∫—É Python...")
        try:
            async with aiohttp.ClientSession() as session:
                async with session.get('https://www.python.org/', timeout=aiohttp.ClientTimeout(total=10)) as resp:
                    html = await resp.text()
                    soup = BeautifulSoup(html, "html.parser")
                    text = soup.get_text(separator="\n", strip=True)
                    lines = [line.strip() for line in text.splitlines() if line.strip() and len(line.strip()) > 10]
                    text = "\n".join(lines[:LIMIT_PARSING])

                    if text:
                        all_texts.append(f"=== –ò—Å—Ç–æ—á–Ω–∏–∫ 1: Python Official Website (python.org) ===\n{text[:2000]}")
                        print(f"{C_GREEN}‚úì{C_RESET} –ü–æ–ª—É—á–µ–Ω–æ —Å python.org: {len(text)} —Å–∏–º–≤–æ–ª–æ–≤")
                        combined = "\n\n".join(all_texts)
                        return combined[:WEB_SEARCH_MAX_LENGTH]
        except Exception as e:
            print(f"{C_YELLOW}[WARN]{C_RESET} –ü—Ä—è–º–æ–π –∑–∞–ø—Ä–æ—Å –Ω–µ —É–¥–∞–ª—Å—è: {e}, –ø–µ—Ä–µ–∫–ª—é—á–∞—é—Å—å –Ω–∞ –ø–æ–∏—Å–∫...")

    # –ß–µ—Ä–Ω—ã–π —Å–ø–∏—Å–æ–∫ –¥–æ–º–µ–Ω–æ–≤ (—Ä–∞–∑–≤–ª–µ–∫–∞—Ç–µ–ª—å–Ω—ã–µ + –ö–ò–¢–ê–ô–°–ö–ò–ï + –§–û–†–£–ú–´)
    blocked_domains = {
        # –†–∞–∑–≤–ª–µ–∫–∞—Ç–µ–ª—å–Ω—ã–µ
        'rutube.ru', 'youtube.com', 'youtu.be', 'kinopoisk.ru',
        'vk.com', 'ok.ru', 'tiktok.com', 'instagram.com', 'facebook.com',
        'genius.com', 'wikislovary.ru', 'wiktionary.org', 'urban dictionary',
        'pinterest.com', 'twitter.com', 'x.com',
        # –ö–ò–¢–ê–ô–°–ö–ò–ï –î–û–ú–ï–ù–´
        'zhihu.com', 'baidu.com', 'weibo.com', 'qq.com', 'taobao.com',
        'tmall.com', 'jd.com', 'sina.com.cn', 'sohu.com', '163.com',
        'douban.com', 'bilibili.com', 'csdn.net', 'cnblogs.com',
        'jianshu.com', 'oschina.net', 'iteye.com', 'segmentfault.com',
        'juejin.cn', 'toutiao.com', 'aliyun.com', 'huawei.com',
        'xiaomi.com', 'oppo.com', 'vivo.com',
        # –§–û–†–£–ú–´ –ò –ù–ï–û–§–ò–¶–ò–ê–õ–¨–ù–´–ï –ò–°–¢–û–ß–ù–ò–ö–ò
        'alkad.org'
    }

    # –ü–∞—Ç—Ç–µ—Ä–Ω—ã –º—É—Å–æ—Ä–Ω—ã—Ö —Å–∞–π—Ç–æ–≤
    blocked_patterns = [
        '–∫–∞–∫ –ø–∏—à–µ—Ç—Å—è', '–ø–µ—Å–Ω—è', '—Ç–µ–∫—Å—Ç –ø–µ—Å–Ω–∏', 'lyrics', '—Ñ–∏–ª—å–º',
        '—Å–º–æ—Ç—Ä–µ—Ç—å –æ–Ω–ª–∞–π–Ω', '—Ç—Ä–µ–π–ª–µ—Ä', 'wiki/–ø–æ—Å–ª–µ–¥–Ω—è—è', 'wiki/–ø–æ—Å–ª–µ–¥–Ω–∏–π',
        '–∑–Ω–∞—á–µ–Ω–∏–µ —Å–ª–æ–≤–∞', '–ø–µ—Ä–µ–≤–æ–¥', '—Å–ª–æ–≤–∞—Ä—å', '—á—Ç–æ –∑–Ω–∞—á–∏—Ç',
        '—Ñ–æ—Ä—É–º', '–æ–±—Å—É–∂–¥–µ–Ω–∏–µ'
    ]

    try:
        query_lower = query.lower()
        enhanced_query = query
        replacements = {
                    '–ø–æ—Å–ª–µ–¥–Ω—è—è –≤–µ—Ä—Å–∏—è': 'latest version',
                    '–ø–æ—Å–ª–µ–¥–Ω–∏–π': 'latest',
                    '–≤–µ—Ä—Å–∏—è': 'version',
                    '–∫–∞–∫–∞—è': 'what',
                    '–∫–∞–∫–æ–π': 'what'
                }
        english_keywords = {'version', 'latest', 'release', 'notes', 'changes', 'stable', 'programming'}
        match None:
            case _ if any(kw in query_lower for kw in english_keywords):
                pass
            case _:
                for rus, eng in replacements.items():
                    if rus in query_lower:
                        enhanced_query = enhanced_query.replace(rus,eng)
        print(f"{C_GRAY}[WEB]{C_RESET} –ó–∞–ø—Ä–æ—Å –∫ –ø–æ–∏—Å–∫—É: {enhanced_query}")

        # –ü–æ–ª—É—á–∞–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
        search_region = 'wt-wt'  # –±–µ–∑ —Ä–µ–≥–∏–æ–Ω–∞–ª—å–Ω—ã—Ö —Ñ–∏–ª—å—Ç—Ä–æ–≤

        print(f"{C_GRAY}[WEB]{C_RESET} DuckDuckGo –ø–æ–∏—Å–∫ (—Ä–µ–≥–∏–æ–Ω: {search_region})...")

        results = await loop.run_in_executor(
            None,
            lambda: list(DDGS().text(enhanced_query, max_results=30, region=search_region))
        )

        print(f"{C_GRAY}[WEB]{C_RESET} –ù–∞–π–¥–µ–Ω–æ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤: {len(results)}")

        if not results:
            return "–ù–∏—á–µ–≥–æ –Ω–µ –Ω–∞–π–¥–µ–Ω–æ –≤ –ø–æ–∏—Å–∫–æ–≤–æ–π —Å–∏—Å—Ç–µ–º–µ. –ü–æ–ø—Ä–æ–±—É–π—Ç–µ –ø–µ—Ä–µ—Ñ–æ—Ä–º—É–ª–∏—Ä–æ–≤–∞—Ç—å –∑–∞–ø—Ä–æ—Å."

        print(f"{C_GRAY}[WEB]{C_RESET} –ù–∞—á–∏–Ω–∞—é —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏—é —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤...")

        # –§–∏–ª—å—Ç—Ä–∞—Ü–∏—è —Å –ø—Ä–∏–æ—Ä–∏—Ç–∏–∑–∞—Ü–∏–µ–π
        all_valid_results = []
        blocked_count = {'chinese_domain': 0, 'chinese_title': 0, 'patterns': 0, 'invalid_url': 0, 'forums': 0}

        # –û—Ñ–∏—Ü–∏–∞–ª—å–Ω—ã–µ –¥–æ–º–µ–Ω—ã –¥–ª—è –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç–∞ (—Å –≤–µ—Å–∞–º–∏)
        priority_domains = {
            'rust-lang.org': 100,
            'doc.rust-lang.org': 100,
            'blog.rust-lang.org': 90,
            'github.com/rust-lang': 85,
            'python.org': 100,
            'docs.python.org': 100,
            'nodejs.org': 100,
            'developer.mozilla.org': 95,
            'golang.org': 100,
            'go.dev': 100,
            'docs.oracle.com': 90,
            'openjdk.org': 90,
            'wikipedia.org': 90,
            'en.wikipedia.org': 70,
            'reddit.com': 95,
            'habr.com': 80,
            'stackoverflow.com': 95
        }

        for r in results:
            title = r.get("title", "")
            href = r.get("href", "")
            body = r.get("body", "")

            # –ü—Ä–æ–≤–µ—Ä—è–µ–º URL
            if not href.startswith(('http://', 'https://')):
                blocked_count['invalid_url'] += 1
                continue

            # –ü—Ä–æ–≤–µ—Ä—è–µ–º –¥–æ–º–µ–Ω
            parsed = urlparse(href)
            domain = parsed.netloc.lower()

            if domain.startswith('www.'):
                domain = domain[4:]

            if not domain or '.' not in domain:
                blocked_count['invalid_url'] += 1
                continue

            # –ë–õ–û–ö–ò–†–£–ï–ú —Ñ–æ—Ä—É–º—ã
            if 'forum' in domain or '—Ñ–æ—Ä—É–º' in title.lower():
                blocked_count['forums'] += 1
                continue

            # –ë–õ–û–ö–ò–†–£–ï–ú –¥–æ–º–µ–Ω—ã —Å –∫–∏—Ç–∞–π—Å–∫–∏–º–∏ —Ä–∞—Å—à–∏—Ä–µ–Ω–∏—è–º–∏
            if domain.endswith(('.cn', '.com.cn')):
                blocked_count['chinese_domain'] += 1
                continue

            # –ë–õ–û–ö–ò–†–£–ï–ú –∏–∑–≤–µ—Å—Ç–Ω—ã–µ –∫–∏—Ç–∞–π—Å–∫–∏–µ/–º—É—Å–æ—Ä–Ω—ã–µ –¥–æ–º–µ–Ω—ã
            if any(blocked in domain for blocked in blocked_domains):
                blocked_count['chinese_domain'] += 1
                continue

            # –ë–õ–û–ö–ò–†–£–ï–ú –µ—Å–ª–∏ –≤ –¥–æ–º–µ–Ω–µ –µ—Å—Ç—å –∫–∏—Ç–∞–π—Å–∫–∏–µ —Å–∏–º–≤–æ–ª—ã
            if any('\u4e00' <= char <= '\u9fff' for char in domain):
                blocked_count['chinese_domain'] += 1
                continue

            # –ü—Ä–æ–≤–µ—Ä—è–µ–º –∑–∞–≥–æ–ª–æ–≤–æ–∫ –Ω–∞ –∫–∏—Ç–∞–π—Å–∫–∏–µ —Å–∏–º–≤–æ–ª—ã
            chinese_in_title = sum(1 for char in title if '\u4e00' <= char <= '\u9fff')
            if chinese_in_title > 0:
                blocked_count['chinese_title'] += 1
                continue

            # –ü—Ä–æ–≤–µ—Ä—è–µ–º –ø–∞—Ç—Ç–µ—Ä–Ω—ã –≤ –∑–∞–≥–æ–ª–æ–≤–∫–µ –∏ –æ–ø–∏—Å–∞–Ω–∏–∏
            title_lower = title.lower()
            body_lower = body.lower()

            if any(pattern in title_lower or pattern in body_lower for pattern in blocked_patterns):
                blocked_count['patterns'] += 1
                continue

            # –û–ø—Ä–µ–¥–µ–ª—è–µ–º –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç
            priority_score = 0
            for priority_domain, score in priority_domains.items():
                if priority_domain in domain or priority_domain in href:
                    priority_score = score
                    break

            # –î–æ–±–∞–≤–ª—è–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç —Å –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç–æ–º
            all_valid_results.append({
                'result': r,
                'priority': priority_score,
                'domain': domain,
                'title': title
            })

        # –°–æ—Ä—Ç–∏—Ä—É–µ–º: —Å–Ω–∞—á–∞–ª–∞ –ø–æ –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç—É (—É–±—ã–≤–∞–Ω–∏–µ), –ø–æ—Ç–æ–º –ø–æ –ø–æ—Ä—è–¥–∫—É
        all_valid_results.sort(key=lambda x: (-x['priority'], results.index(x['result'])))

        # –ë–µ—Ä–µ–º —Ç–æ–ø —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
        final_results = [item['result'] for item in all_valid_results[:WEB_SEARCH_MAX_RESULTS]]

        # –í—ã–≤–æ–¥–∏–º –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é
        print(f"{C_GRAY}[WEB]{C_RESET} –û—Ç—Ñ–∏–ª—å—Ç—Ä–æ–≤–∞–Ω–æ: –¥–æ–º–µ–Ω—ã={blocked_count['chinese_domain']}, –∑–∞–≥–æ–ª–æ–≤–∫–∏={blocked_count['chinese_title']}, –ø–∞—Ç—Ç–µ—Ä–Ω—ã={blocked_count['patterns']}, —Ñ–æ—Ä—É–º—ã={blocked_count['forums']}, –Ω–µ–∫–æ—Ä—Ä–µ–∫—Ç–Ω—ã–µ={blocked_count['invalid_url']}")

        priority_count = sum(1 for item in all_valid_results[:WEB_SEARCH_MAX_RESULTS] if item['priority'] > 0)
        print(f"{C_GRAY}[WEB]{C_RESET} –í—ã–±—Ä–∞–Ω–æ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤: {len(final_results)} (–ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç–Ω—ã—Ö: {priority_count})")

        # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º —á—Ç–æ –≤—ã–±—Ä–∞–Ω–æ
        for i, item in enumerate(all_valid_results[:WEB_SEARCH_MAX_RESULTS], 1):
            if item['priority'] > 0:
                print(f"{C_GREEN}[‚òÖ {item['priority']}]{C_RESET} {item['title'][:60]}... ({item['domain']})")
            else:
                print(f"{C_CYAN}[OK]{C_RESET} {item['title'][:60]}... ({item['domain']})")

        if not final_results:
            return f"–ù–µ —É–¥–∞–ª–æ—Å—å –Ω–∞–π—Ç–∏ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã. –í—Å–µ–≥–æ –Ω–∞–π–¥–µ–Ω–æ: {len(results)}, –∑–∞–±–ª–æ–∫–∏—Ä–æ–≤–∞–Ω–æ: {sum(blocked_count.values())}"

        # –°–æ–∑–¥–∞–µ–º connector —Å —É–≤–µ–ª–∏—á–µ–Ω–Ω—ã–º —Ç–∞–π–º–∞—É—Ç–æ–º –¥–ª—è –ø–æ–¥–∫–ª—é—á–µ–Ω–∏—è
        timeout = aiohttp.ClientTimeout(
            total=WEB_SEARCH_TIMEOUT,
            connect=5,  # 5 —Å–µ–∫—É–Ω–¥ –Ω–∞ –ø–æ–¥–∫–ª—é—á–µ–Ω–∏–µ
            sock_read=5  # 5 —Å–µ–∫—É–Ω–¥ –Ω–∞ —á—Ç–µ–Ω–∏–µ —Å–æ–∫–µ—Ç–∞
        )

        connector = aiohttp.TCPConnector(
            limit=10,
            limit_per_host=3,
            ttl_dns_cache=300
        )

        async with aiohttp.ClientSession(timeout=timeout, connector=connector) as session:
            for i, result in enumerate(final_results, 1):
                url = result.get("href")
                title = result.get("title", "–ë–µ–∑ –Ω–∞–∑–≤–∞–Ω–∏—è")

                print(f"{C_GRAY}[WEB]{C_RESET} [{i}/{len(final_results)}] {title[:50]}...")

                try:
                    async with session.get(
                        url,
                        headers={
                            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36',
                            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8',
                            'Accept-Language': 'en-US,en;q=0.9',
                            'Accept-Encoding': 'gzip, deflate',
                        },
                        allow_redirects=True,
                        max_redirects=2
                    ) as resp:

                        content_type = resp.headers.get('content-type', '').lower()
                        if 'text/html' not in content_type:
                            all_texts.append(f"=== –ò—Å—Ç–æ—á–Ω–∏–∫ {i}: {title} ===\n[–ù–µ HTML: {content_type}]")
                            continue

                        html = await resp.text(errors='replace')

                    # –ü–∞—Ä—Å–∏–Ω–≥
                    soup = BeautifulSoup(html, "html.parser")

                    for tag in soup(["script", "style", "nav", "footer", "header", "aside", "form", "iframe", "noscript"]):
                        tag.decompose()

                    # –ò—â–µ–º –∫–æ–Ω—Ç–µ–Ω—Ç
                    main_content = (
                        soup.find('main') or
                        soup.find('article') or
                        soup.find('div', class_=re.compile('content|main|article|post')) or
                        soup.find('div', id=re.compile('content|main|article'))
                    )

                    if main_content:
                        text = main_content.get_text(separator="\n", strip=True)
                    else:
                        text = soup.get_text(separator="\n", strip=True)

                    # –§–ò–õ–¨–¢–†–£–ï–ú –∫–∏—Ç–∞–π—Å–∫–∏–µ —Å–∏–º–≤–æ–ª—ã –∏–∑ —Ç–µ–∫—Å—Ç–∞
                    lines = []
                    for line in text.splitlines():
                        line = line.strip()
                        # –ü—Ä–æ–ø—É—Å–∫–∞–µ–º —Å—Ç—Ä–æ–∫–∏ —Å –±–æ–ª—å—à–∏–º –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ–º –∫–∏—Ç–∞–π—Å–∫–∏—Ö —Å–∏–º–≤–æ–ª–æ–≤
                        chinese_chars = sum(1 for char in line if '\u4e00' <= char <= '\u9fff')
                        if chinese_chars > len(line) * 0.3:  # –ï—Å–ª–∏ >30% –∫–∏—Ç–∞–π—Å–∫–∏—Ö —Å–∏–º–≤–æ–ª–æ–≤
                            continue
                        if line and len(line) > 20:
                            lines.append(line)

                    text = "\n".join(lines[:50])  # –£–≤–µ–ª–∏—á–µ–Ω–æ —Å 25 –¥–æ 50 —Å—Ç—Ä–æ–∫

                    if len(text) < 50:
                        all_texts.append(f"=== –ò—Å—Ç–æ—á–Ω–∏–∫ {i}: {title} ===\n[–°–æ–¥–µ—Ä–∂–∏–º–æ–µ —Å–ª–∏—à–∫–æ–º –∫–æ—Ä–æ—Ç–∫–æ–µ]")
                        continue

                    all_texts.append(f"=== –ò—Å—Ç–æ—á–Ω–∏–∫ {i}: {title} ({url}) ===\n{text[:2500]}")  # –£–≤–µ–ª–∏—á–µ–Ω–æ —Å 1800 –¥–æ 2500
                    print(f"{C_GREEN}‚úì{C_RESET} {len(text)} —Å–∏–º–≤–æ–ª–æ–≤")

                except asyncio.TimeoutError:
                    print(f"{C_YELLOW}‚ö†{C_RESET} –¢–∞–π–º–∞—É—Ç (–ø—Ä–æ–ø—É—Å–∫–∞–µ–º)")
                    # –ù–ï –¥–æ–±–∞–≤–ª—è–µ–º –≤ all_texts, –ø—Ä–æ—Å—Ç–æ –ø—Ä–æ–ø—É—Å–∫–∞–µ–º
                    continue
                except aiohttp.ClientError as e:
                    print(f"{C_YELLOW}‚ö†{C_RESET} –û—à–∏–±–∫–∞ —Å–æ–µ–¥–∏–Ω–µ–Ω–∏—è (–ø—Ä–æ–ø—É—Å–∫–∞–µ–º)")
                    continue
                except Exception as e:
                    print(f"{C_YELLOW}‚ö†{C_RESET} –û—à–∏–±–∫–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏ (–ø—Ä–æ–ø—É—Å–∫–∞–µ–º)")
                    continue

        if not all_texts:
            return "–ù–µ —É–¥–∞–ª–æ—Å—å –ø–æ–ª—É—á–∏—Ç—å –¥–∞–Ω–Ω—ã–µ —Å —Å–∞–π—Ç–æ–≤ (–≤—Å–µ –∏—Å—Ç–æ—á–Ω–∏–∫–∏ –Ω–µ–¥–æ—Å—Ç—É–ø–Ω—ã –∏–ª–∏ –ø–æ —Ç–∞–π–º–∞—É—Ç—É)."

        combined = "\n\n".join(all_texts)
        if len(combined) > WEB_SEARCH_MAX_LENGTH:
            combined = combined[:WEB_SEARCH_MAX_LENGTH] + f"\n\n... [–û–±—Ä–µ–∑–∞–Ω–æ –¥–æ {WEB_SEARCH_MAX_LENGTH} —Å–∏–º–≤–æ–ª–æ–≤]"

        print(f"{C_GRAY}[WEB]{C_RESET} –ò—Ç–æ–≥–æ –≤–æ–∑–≤—Ä–∞—â–∞—é: {len(combined)} —Å–∏–º–≤–æ–ª–æ–≤ –∏–∑ {len(all_texts)} –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤")
        return combined

    except Exception as e:
        error_msg = f"–ö—Ä–∏—Ç–∏—á–µ—Å–∫–∞—è –æ—à–∏–±–∫–∞ –ø–æ–∏—Å–∫–∞: {type(e).__name__}: {str(e)}"
        print(f"{C_RED}[ERROR]{C_RESET} {error_msg}")
        return error_msg

def print_header():
    print(f"\n{C_GRAY}{'='*60}{C_RESET}")
    print(f"{C_BLUE}üõ†  AI Project Manager v5.5{C_RESET} {C_GRAY}|{C_RESET} Smart Search")
    print(f"{C_GRAY}{'='*60}{C_RESET}\n")


def print_help():
    print(f"{C_CYAN}–ö–æ–º–∞–Ω–¥—ã:{C_RESET}")
    print(f"  {C_YELLOW}/info{C_RESET}                          {C_GRAY}–ü–∞–Ω–µ–ª—å –∫–æ–º–∞–Ω–¥{C_RESET}")
    print(f"  {C_YELLOW}/create <name> <path> <goal>{C_RESET}   {C_GRAY}–°–æ–∑–¥–∞—Ç—å –ø—Ä–æ–µ–∫—Ç{C_RESET}")
    print(f"  {C_YELLOW}/list{C_RESET}                          {C_GRAY}–°–ø–∏—Å–æ–∫ –ø—Ä–æ–µ–∫—Ç–æ–≤{C_RESET}")
    print(f"  {C_YELLOW}/load <name>{C_RESET}                   {C_GRAY}–ó–∞–≥—Ä—É–∑–∏—Ç—å –ø—Ä–æ–µ–∫—Ç{C_RESET}")
    print(f"  {C_YELLOW}/delete <name>{C_RESET}                 {C_GRAY}–£–¥–∞–ª–∏—Ç—å –ø—Ä–æ–µ–∫—Ç{C_RESET}")
    print(f"  {C_YELLOW}/doc <directory>{C_RESET}               {C_GRAY}–ü—Ä–∏–∫—Ä–µ–ø–∏—Ç—å –∫–∞—Ç–∞–ª–æ–≥ –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏–∏{C_RESET}")
    print(f"  {C_YELLOW}/doc_del{C_RESET}                       {C_GRAY}–£–¥–∞–ª–∏—Ç—å –ø—É—Ç—å –∫ –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏–∏{C_RESET}")
    print(f"  {C_YELLOW}/analyze{C_RESET}                       {C_GRAY}–†–µ–∂–∏–º –∞–Ω–∞–ª–∏–∑–∞{C_RESET}")
    print(f"  {C_YELLOW}/analyze_prompt <text>{C_RESET}         {C_GRAY}–°–æ—Ö—Ä–∞–Ω–∏—Ç—å –ø—Ä–æ–º–ø—Ç{C_RESET}")
    print(f"  {C_YELLOW}/architect <text>{C_RESET}              {C_GRAY}–°–æ—Ö—Ä–∞–Ω–∏—Ç—å –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä—É{C_RESET}")
    print(f"  {C_YELLOW}/dev{C_RESET}                           {C_GRAY}–†–µ–∂–∏–º —Ä–∞–∑—Ä–∞–±–æ—Ç–∫–∏{C_RESET}")
    print(f"  {C_YELLOW}/review <file>{C_RESET}                 {C_GRAY}–†–µ–≤—å—é –∫–æ–¥–∞{C_RESET}")
    print(f"  {C_YELLOW}/explain <file>{C_RESET}                {C_GRAY}–û–±—ä—è—Å–Ω–∏—Ç—å –∫–æ–¥{C_RESET}")
    print(f"  {C_YELLOW}/dialog_web <question>{C_RESET}         {C_GRAY}–î–∏–∞–ª–æ–≥ —Å –ò–ò (—Å –≤–µ–±-–ø–æ–∏—Å–∫–æ–º){C_RESET}")
    print(f"  {C_YELLOW}/dialog_status{C_RESET}                 {C_GRAY}–ü–æ–∫–∞–∑–∞—Ç—å —Å—Ç–∞—Ç—É—Å –¥–∏–∞–ª–æ–≥–∞{C_RESET}")
    print(f"  {C_YELLOW}/dialog_clean{C_RESET}                  {C_GRAY}–û—á–∏—Å—Ç–∏—Ç—å –∏—Å—Ç–æ—Ä–∏—é –¥–∏–∞–ª–æ–≥–∞{C_RESET}")
    print(f"  {C_YELLOW}/close{C_RESET}                         {C_GRAY}–°–æ—Ö—Ä–∞–Ω–∏—Ç—å –∏ –≤—ã–π—Ç–∏{C_RESET}")
    print(f"  {C_YELLOW}/exit{C_RESET}                          {C_GRAY}–í—ã—Ö–æ–¥{C_RESET}")
    print(f"  {C_YELLOW}/ant <question>{C_RESET}              {C_GRAY}–î–∏–∞–ª–æ–≥ —á–µ—Ä–µ–∑ Anthropic SDK (–∫–∞–∫ ant.py){C_RESET}")
    print(f"\n{C_GRAY}–ù–∞—Å—Ç—Ä–æ–π–∫–∏ –ø–æ–∏—Å–∫–∞: {WEB_SEARCH_MAX_RESULTS} —Å–∞–π—Ç–æ–≤, {WEB_SEARCH_MAX_LENGTH} —Å–∏–º–≤–æ–ª–æ–≤, —Ç–∞–π–º–∞—É—Ç {WEB_SEARCH_TIMEOUT}—Å{C_RESET}")
    print(f"{C_GRAY}–ù–∞—Å—Ç—Ä–æ–π–∫–∏ –¥–∏–∞–ª–æ–≥–∞: –º–∞–∫—Å. –∏—Ç–µ—Ä–∞—Ü–∏–π {DIALOG_MAX_ITERATIONS}{C_RESET}")
    print()
